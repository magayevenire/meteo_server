{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark \n",
    "findspark.init()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "import shutil\n",
    "import glob\n",
    "\n",
    "from configparser import ConfigParser\n",
    "import mysql.connector\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "session démarrée, son id est  local-1685284060978\n"
     ]
    }
   ],
   "source": [
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Data_Pipeline\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\"session démarrée, son id est \", sc.applicationId)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['./Config.ini']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = ConfigParser()\n",
    "config.read('./Config.ini')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "url = config.get('mysql', 'host')\n",
    "user = config.get('mysql', 'user')\n",
    "pwd  = config.get('mysql', 'password')\n",
    "db =  config.get('mysql', 'database')\n",
    "pt = config.get('mysql', 'port')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mouha\\AppData\\Local\\Temp\\ipykernel_19764\\3200221813.py:10: UserWarning: pandas only supports SQLAlchemy connectable (engine/connection) or database string URI or sqlite3 DBAPI2 connection. Other DBAPI2 objects are not tested. Please consider using SQLAlchemy.\n",
      "  pdf = pd.read_sql(query, con=conn)\n"
     ]
    }
   ],
   "source": [
    "# Etablir une connexion à MySQL\n",
    "conn = mysql.connector.connect(user= user, database=db,\n",
    "                               password=pwd,\n",
    "                               host=url,\n",
    "                               port=pt)\n",
    "cursor = conn.cursor()\n",
    "query = \"select * from data\"\n",
    "\n",
    "# Execution de la requête et sauvegarde du résultat dans un dataframe pandas\n",
    "pdf = pd.read_sql(query, con=conn)\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>city</th>\n",
       "      <th>temperature</th>\n",
       "      <th>humidity</th>\n",
       "      <th>windSpeed</th>\n",
       "      <th>timestamp</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>84</td>\n",
       "      <td>54</td>\n",
       "      <td>162</td>\n",
       "      <td>2023-05-28 14:44:43</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>29</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-05-28 16:59:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>29</td>\n",
       "      <td>65</td>\n",
       "      <td>8</td>\n",
       "      <td>2023-05-28 16:59:44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>Dakar</td>\n",
       "      <td>28</td>\n",
       "      <td>69</td>\n",
       "      <td>9</td>\n",
       "      <td>2023-05-28 18:06:12</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id   city  temperature  humidity  windSpeed           timestamp\n",
       "0   1  Dakar           84        54        162 2023-05-28 14:44:43\n",
       "1   2  Dakar           29        65          8 2023-05-28 16:59:44\n",
       "2   3  Dakar           29        65          8 2023-05-28 16:59:44\n",
       "3   4  Dakar           28        69          9 2023-05-28 18:06:12"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "APIKEY = config.get(\"openweathermap\", \"APIKEY\")\n",
    "URL = config.get(\"openweathermap\", \"URL\")\n",
    "\n",
    "city = {\n",
    "  \"name\": \"Dakar\",\n",
    "  \"lat\": 14.67,\n",
    "  \"lon\": -17.44\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'coord': {'lon': -17.44, 'lat': 14.67}, 'weather': [{'id': 800, 'main': 'Clear', 'description': 'clear sky', 'icon': '01d'}], 'base': 'stations', 'main': {'temp': 27.99, 'feels_like': 30.52, 'temp_min': 27.99, 'temp_max': 32.42, 'pressure': 1008, 'humidity': 69}, 'visibility': 10000, 'wind': {'speed': 8.75, 'deg': 360}, 'clouds': {'all': 0}, 'dt': 1685297172, 'sys': {'type': 1, 'id': 2419, 'country': 'SN', 'sunrise': 1685255993, 'sunset': 1685302467}, 'timezone': 0, 'id': 2595757, 'name': 'Central Dakar', 'cod': 200}\n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "import json\n",
    "from datetime import datetime\n",
    "# url = \"https://api.open-meteo.com/v1/forecast?latitude=14.69&longitude=-17.44&hourly=temperature_2m,relativehumidity_2m,windspeed_10m\"\n",
    "api_url = f\"{URL}?lat={city['lat']}&lon={city['lon']}&appid={APIKEY}&units=metric\"\n",
    "\n",
    "data = requests.get(api_url)\n",
    "data_parsed = json.loads(data.text)\n",
    "print(data_parsed)\n",
    "data_formatted = {\n",
    "  \"city\": city['name'],\n",
    "  \"temperature\": data_parsed['main']['temp'],\n",
    "  \"humidity\": data_parsed['main']['humidity'],\n",
    "  \"windSpeed\": data_parsed['wind']['speed'],\n",
    "  \"timestamp\": datetime.utcfromtimestamp(data_parsed['dt']).strftime('%Y-%m-%d %H:%M:%S'),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+----+-----------+--------+---------+---------+\n",
      "|  id|city|temperature|humidity|windSpeed|timestamp|\n",
      "+----+----+-----------+--------+---------+---------+\n",
      "|null|null|       null|    null|     null|     null|\n",
      "|null|null|       null|    null|     null|     null|\n",
      "|null|null|       null|    null|     null|     null|\n",
      "+----+----+-----------+--------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "# Puisque nous connaissons déjà le format des données, définissons le schéma pour accélérer le traitement (pas besoin de Spark pour déduire le schéma)\n",
    "jsonSchema = StructType()\\\n",
    "    .add(\"id\", StringType())\\\n",
    "    .add(\"city\", StringType(), True)\\\n",
    "    .add(\"temperature\", StringType(), True)\\\n",
    "    .add(\"humidity\", StringType(), True)\\\n",
    "    .add(\"windSpeed\", StringType(), True)\\\n",
    "    .add(\"timestamp\", StringType())\\\n",
    "# DataFrame statique représentant les données dans les fichiers JSON\n",
    "staticInputDF = (\n",
    "  spark\n",
    "    .read\n",
    "    .schema(jsonSchema)\n",
    "    .json(df.rdd)\n",
    ")\n",
    "\n",
    "staticInputDF.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Record inserted successfully into data table\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "  conn = mysql.connector.connect(user= user, database=db, password=pwd, host=url, port=pt)\n",
    "  cursor = conn.cursor()\n",
    "  query = f\"\"\"\n",
    "  INSERT INTO `data` (`city`, `temperature`, `humidity`, `windSpeed`, `timestamp`) \n",
    "  VALUES (\n",
    "    '{data_formatted['city']}', \n",
    "    '{data_formatted['temperature']}', \n",
    "    '{data_formatted['humidity']}', \n",
    "    '{data_formatted['windSpeed']}', \n",
    "    '{data_formatted['timestamp']}'\n",
    "  )\"\"\"\n",
    "\n",
    "  cursor.execute(query)\n",
    "  conn.commit()\n",
    "  print(cursor.rowcount, \"Record inserted successfully into data table\")\n",
    "  conn.close()\n",
    "except mysql.connector.Error as error:\n",
    "  print(\"Failed to insert record into Laptop table {}\".format(error))\n",
    "\n",
    "finally:\n",
    "  if conn.is_connected():\n",
    "    conn.close()\n",
    "    print(\"MySQL connection is closed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "from pyspark.sql.functions import udf, col, explode\n",
    "from pyspark.sql.types import StructType, StructField, IntegerType, StringType, ArrayType"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+----+\n",
      "|verb|                 url|             headers|body|\n",
      "+----+--------------------+--------------------+----+\n",
      "| get|https://api.openw...|{content-type -> ...|  {}|\n",
      "+----+--------------------+--------------------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import Row\n",
    "headers = {\n",
    "  'content-type': \"application/json\"\n",
    "}\n",
    "body = json.dumps({})\n",
    "RestApiRequestRow = Row(\"verb\", \"url\", \"headers\", \"body\")\n",
    "\n",
    "api_url = \"https://api.openweathermap.org/data/2.5/onecall?lat=14.497401&lon=-14.452362&exclude=hourly,daily&appid=101098fb7b42c64a657c60649632e063\"\n",
    "request_df = spark.createDataFrame([\n",
    "  RestApiRequestRow(\"get\", api_url, headers, body)\n",
    "])\n",
    "\n",
    "request_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"lat\", DoubleType(), True),\n",
    "    StructField(\"lon\", DoubleType(), True),\n",
    "    StructField(\"timezone\", StringType(), True),\n",
    "    StructField(\"timezone_offset\", IntegerType(), True),\n",
    "    StructField(\"current\", StructType([\n",
    "        StructField(\"dt\", LongType(), True),\n",
    "        StructField(\"sunrise\", LongType(), True),\n",
    "        StructField(\"sunset\", LongType(), True),\n",
    "        StructField(\"temp\", DoubleType(), True),\n",
    "        StructField(\"feels_like\", DoubleType(), True),\n",
    "        StructField(\"pressure\", IntegerType(), True),\n",
    "        StructField(\"humidity\", IntegerType(), True),\n",
    "        StructField(\"dew_point\", DoubleType(), True),\n",
    "        StructField(\"uvi\", IntegerType(), True),\n",
    "        StructField(\"clouds\", IntegerType(), True),\n",
    "        StructField(\"visibility\", IntegerType(), True),\n",
    "        StructField(\"wind_speed\", DoubleType(), True),\n",
    "        StructField(\"wind_deg\", IntegerType(), True),\n",
    "        StructField(\"wind_gust\", DoubleType(), True),\n",
    "        StructField(\"weather\", ArrayType(StructType([\n",
    "            StructField(\"id\", IntegerType(), True),\n",
    "            StructField(\"main\", StringType(), True),\n",
    "            StructField(\"description\", StringType(), True),\n",
    "            StructField(\"icon\", StringType(), True)\n",
    "        ])), True)\n",
    "    ]), True)\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def executeRestApi(verb, url, headers, body):\n",
    "  #\n",
    "  headers = {\n",
    "      'content-type': \"application/json\"\n",
    "  }\n",
    "  res = None\n",
    "  # Make API request, get response object back, create dataframe from above schema.\n",
    "  try:\n",
    "    if verb == \"get\":\n",
    "      res = requests.get(url, data=body, headers=headers)\n",
    "    else:\n",
    "      res = requests.post(url, data=body, headers=headers)\n",
    "  except Exception as e:\n",
    "    return e\n",
    "  if res != None and res.status_code == 200:\n",
    "    return json.loads(res.text)\n",
    "  return None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<function __main__.executeRestApi(verb, url, headers, body)>"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf_executeRestApi = udf(executeRestApi, schema)\n",
    "udf_executeRestApi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Column<'executeRestApi(verb, url, headers, body)'>"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "udf_executeRestApi(col(\"verb\"), col(\"url\"), col(\"headers\"), col(\"body\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = request_df \\\n",
    "             .withColumn(\"data\", udf_executeRestApi(col(\"verb\"), col(\"url\"), col(\"headers\"), col(\"body\")))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-------------+-----------------+------------------+---------+---------+----+\n",
      "|address_city|address_state|address_zip_first|address_zip_second|car_color|car_model|name|\n",
      "+------------+-------------+-----------------+------------------+---------+---------+----+\n",
      "|     Houston|        Texas|             1234|              4321|      red|   jaguar|  Jo|\n",
      "+------------+-------------+-----------------+------------------+---------+---------+----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "# from pyspark.dbutils import DBUtils\n",
    "from pyspark.sql.functions import col\n",
    "from pyspark.sql.types import StructType\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# declare dummy data to demonstrate how the collapse mechanism works\n",
    "jsonStrings = ['{\"car\":{\"color\":\"red\", \"model\":\"jaguar\"},\"name\":\"Jo\",\"address\":{\"city\":\"Houston\",\"state\":\"Texas\", \"zip\":{\"first\":1234,\"second\":4321}}}']\n",
    "otherPeopleRDD = spark.sparkContext.parallelize(jsonStrings)\n",
    "df = spark.read.json(otherPeopleRDD)\n",
    "  \n",
    "# Recursively iterates over the schema, creating an array of arrays, whereby each item\n",
    "# of the master array, is an array of column names\n",
    "#\n",
    "# For example, lets say there are three columns of which two are hierarchical and the following schema/structure\n",
    "#    name\n",
    "#    address\n",
    "#      street\n",
    "#      town\n",
    "#    details\n",
    "#      age\n",
    "#      gender\n",
    "#\n",
    "# The function will return the following array:\n",
    "# [[\"name\"],[\"address\",\"street\"],[\"address\",\"town\"],[\"details\",\"age\"],[\"details\",\"gender\"]]\n",
    "def get_all_columns_from_schema(source_schema):\n",
    "  branches = []\n",
    "  def inner_get(schema, ancestor=None):\n",
    "    if ancestor is None: ancestor = []\n",
    "    for field in schema.fields:\n",
    "      branch_path = ancestor+[field.name]     \n",
    "      if isinstance(field.dataType, StructType):    \n",
    "        inner_get(field.dataType, branch_path) \n",
    "      else:\n",
    "        branches.append(branch_path)\n",
    "        \n",
    "  inner_get(source_schema)\n",
    "        \n",
    "  return branches\n",
    "\n",
    "# collapse_columns is passed the dataframe schema, which is then passes\n",
    "# to get_all_columns_from_schema.  On return, it iterates through the array\n",
    "# of columns in order to build up the select list that will be used\n",
    "# to collapse the hierarchical columns into a single 2d structure\n",
    "#\n",
    "# for example, lets say _all_columns has the following array: [[\"name\"],[\"address\",\"street\"]]\n",
    "# after iterating through the array, the function response will be\n",
    "# [col(\"name\"), col(\"address.street\").alias(\"address_street\")]\n",
    "def collapse_columns(source_schema, columnFilter=None):\n",
    "  _columns_to_select = []\n",
    "  if columnFilter is None: columnFilter = \"\"\n",
    "  _all_columns = get_all_columns_from_schema(source_schema)\n",
    "  for column_collection in _all_columns:\n",
    "    if (len(columnFilter) > 0) & (column_collection[0] != columnFilter): \n",
    "        continue\n",
    "\n",
    "    # columns with questionable character choices like a space, need to be wrapped\n",
    "    # in `` characters.  The alias function will do this automatically, but the selection of the column\n",
    "    # e.g. col(\"col name\") will not\n",
    "    select_column_collection = ['`%s`' % list_item for list_item in column_collection]    \n",
    "    \n",
    "    if len(column_collection) > 1:\n",
    "      _columns_to_select.append(col('.'.join(select_column_collection)).alias('_'.join(column_collection)))\n",
    "    else:\n",
    "      _columns_to_select.append(col(select_column_collection[0]))\n",
    "\n",
    "  return _columns_to_select\n",
    "\n",
    "# as above but for individual columns\n",
    "def collapse_column(source_df, source_column):\n",
    "    column_name = \"\"\n",
    "    if isinstance(source_column, Column):\n",
    "      column_name = source_column.name\n",
    "    else:\n",
    "      column_name = source_column\n",
    "\n",
    "    return collapse_columns(source_df.schema, column_name)\n",
    "\n",
    "# returns a dataframe that has been collapsed.  Input is the dataframe to be collapsed\n",
    "def collapse_to_dataframe(source_df):\n",
    "  return source_df.select(collapse_columns(source_df.schema))\n",
    "  \n",
    "# now test\n",
    "collapse_to_dataframe(df).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = result_df.select(explode(col(\"result.current\")).alias(\"results\"))\n",
    "# df.select(collapse_columns(df.schema)).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+--------------------+--------------------+----+--------------------+\n",
      "|verb|                 url|             headers|body|                data|\n",
      "+----+--------------------+--------------------+----+--------------------+\n",
      "| get|https://api.openw...|{content-type -> ...|  {}|{14.4974, -14.452...|\n",
      "+----+--------------------+--------------------+----+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "Column `data` has a data type of struct<lat:double,lon:double,timezone:string,timezone_offset:int,current:struct<dt:bigint,sunrise:bigint,sunset:bigint,temp:double,feels_like:double,pressure:int,humidity:int,dew_point:double,uvi:int,clouds:int,visibility:int,wind_speed:double,wind_deg:int,wind_gust:double,weather:array<struct<id:int,main:string,description:string,icon:string>>>>, which is not supported by CSV.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mouha\\Documents\\UIDT\\Traitement De Donnees De Capteurs\\Projet_Final_BigData\\scripts\\get_data.ipynb Cell 22\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X40sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m result_df\u001b[39m.\u001b[39;49mselect(\u001b[39m'\u001b[39;49m\u001b[39mdata\u001b[39;49m\u001b[39m'\u001b[39;49m)\u001b[39m.\u001b[39;49mwrite\u001b[39m.\u001b[39;49mcsv(\u001b[39m\"\u001b[39;49m\u001b[39m./data\u001b[39;49m\u001b[39m\"\u001b[39;49m)\n",
      "File \u001b[1;32m~\\Spark\\python\\pyspark\\sql\\readwriter.py:1799\u001b[0m, in \u001b[0;36mDataFrameWriter.csv\u001b[1;34m(self, path, mode, compression, sep, quote, escape, header, nullValue, escapeQuotes, quoteAll, dateFormat, timestampFormat, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, charToEscapeQuoteEscaping, encoding, emptyValue, lineSep)\u001b[0m\n\u001b[0;32m   1780\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmode(mode)\n\u001b[0;32m   1781\u001b[0m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_set_opts(\n\u001b[0;32m   1782\u001b[0m     compression\u001b[39m=\u001b[39mcompression,\n\u001b[0;32m   1783\u001b[0m     sep\u001b[39m=\u001b[39msep,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1797\u001b[0m     lineSep\u001b[39m=\u001b[39mlineSep,\n\u001b[0;32m   1798\u001b[0m )\n\u001b[1;32m-> 1799\u001b[0m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jwrite\u001b[39m.\u001b[39;49mcsv(path)\n",
      "File \u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Spark\\python\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: Column `data` has a data type of struct<lat:double,lon:double,timezone:string,timezone_offset:int,current:struct<dt:bigint,sunrise:bigint,sunset:bigint,temp:double,feels_like:double,pressure:int,humidity:int,dew_point:double,uvi:int,clouds:int,visibility:int,wind_speed:double,wind_deg:int,wind_gust:double,weather:array<struct<id:int,main:string,description:string,icon:string>>>>, which is not supported by CSV."
     ]
    }
   ],
   "source": [
    "# result_df.select('data').write.csv(\"./data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_select_df = result_df.select('data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "the JSON object must be str, bytes or bytearray, not RDD",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mouha\\Documents\\UIDT\\Traitement De Donnees De Capteurs\\Projet_Final_BigData\\scripts\\get_data.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 7>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X41sZmlsZQ%3D%3D?line=0'>1</a>\u001b[0m \u001b[39m# result_select_df.\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X41sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39m# df=spark.createDataFrame(result_select_df,schema)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X41sZmlsZQ%3D%3D?line=2'>3</a>\u001b[0m \n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X41sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# df.show(5,False)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X41sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mjson\u001b[39;00m\n\u001b[1;32m----> <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X41sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39mprint\u001b[39m(json\u001b[39m.\u001b[39;49mloads(result_select_df\u001b[39m.\u001b[39;49mtoJSON()) )\n",
      "File \u001b[1;32mc:\\Users\\mouha\\AppData\\Local\\Programs\\Python\\Python310\\lib\\json\\__init__.py:339\u001b[0m, in \u001b[0;36mloads\u001b[1;34m(s, cls, object_hook, parse_float, parse_int, parse_constant, object_pairs_hook, **kw)\u001b[0m\n\u001b[0;32m    337\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    338\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(s, (\u001b[39mbytes\u001b[39m, \u001b[39mbytearray\u001b[39m)):\n\u001b[1;32m--> 339\u001b[0m         \u001b[39mraise\u001b[39;00m \u001b[39mTypeError\u001b[39;00m(\u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mthe JSON object must be str, bytes or bytearray, \u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m    340\u001b[0m                         \u001b[39mf\u001b[39m\u001b[39m'\u001b[39m\u001b[39mnot \u001b[39m\u001b[39m{\u001b[39;00ms\u001b[39m.\u001b[39m\u001b[39m__class__\u001b[39m\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m'\u001b[39m)\n\u001b[0;32m    341\u001b[0m     s \u001b[39m=\u001b[39m s\u001b[39m.\u001b[39mdecode(detect_encoding(s), \u001b[39m'\u001b[39m\u001b[39msurrogatepass\u001b[39m\u001b[39m'\u001b[39m)\n\u001b[0;32m    343\u001b[0m \u001b[39mif\u001b[39;00m (\u001b[39mcls\u001b[39m \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    344\u001b[0m         parse_int \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m parse_float \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m\n\u001b[0;32m    345\u001b[0m         parse_constant \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m object_pairs_hook \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m kw):\n",
      "\u001b[1;31mTypeError\u001b[0m: the JSON object must be str, bytes or bytearray, not RDD"
     ]
    }
   ],
   "source": [
    "# result_select_df.\n",
    "# df=spark.createDataFrame(result_select_df,schema)\n",
    "\n",
    "# df.show(5,False)\n",
    "import json\n",
    "\n",
    "print(json.loads(result_select_df.toJSON()) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|              result|\n",
      "+--------------------+\n",
      "|{14.4974, -14.452...|\n",
      "+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "result_df.select('result').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_select_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "ename": "AnalysisException",
     "evalue": "[DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"from_json(result)\" due to data type mismatch: Parameter 1 requires the \"STRING\" type, however \"result\" has the type \"STRUCT<lat: DOUBLE, lon: DOUBLE, timezone: STRING, timezone_offset: INT, current: STRUCT<dt: BIGINT, sunrise: BIGINT, sunset: BIGINT, temp: DOUBLE, feels_like: DOUBLE, pressure: INT, humidity: INT, dew_point: DOUBLE, uvi: INT, clouds: INT, visibility: INT, wind_speed: DOUBLE, wind_deg: INT, wind_gust: DOUBLE, weather: ARRAY<STRUCT<id: INT, main: STRING, description: STRING, icon: STRING>>>>\".;\n'Project [verb#227, url#228, headers#229, body#230, from_json(StructField(lat,DoubleType,true), StructField(lon,DoubleType,true), StructField(timezone,StringType,true), StructField(timezone_offset,IntegerType,true), StructField(current,StructType(StructField(dt,LongType,true),StructField(sunrise,LongType,true),StructField(sunset,LongType,true),StructField(temp,DoubleType,true),StructField(feels_like,DoubleType,true),StructField(pressure,IntegerType,true),StructField(humidity,IntegerType,true),StructField(dew_point,DoubleType,true),StructField(uvi,IntegerType,true),StructField(clouds,IntegerType,true),StructField(visibility,IntegerType,true),StructField(wind_speed,DoubleType,true),StructField(wind_deg,IntegerType,true),StructField(wind_gust,DoubleType,true),StructField(weather,ArrayType(StructType(StructField(id,IntegerType,true),StructField(main,StringType,true),StructField(description,StringType,true),StructField(icon,StringType,true)),true),true)),true), result#311, Some(Etc/UTC)) AS result#417]\n+- Project [verb#227, url#228, headers#229, body#230, executeRestApi(verb#227, url#228, headers#229, body#230)#310 AS result#311]\n   +- LogicalRDD [verb#227, url#228, headers#229, body#230], false\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAnalysisException\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32mc:\\Users\\mouha\\Documents\\UIDT\\Traitement De Donnees De Capteurs\\Projet_Final_BigData\\scripts\\get_data.ipynb Cell 24\u001b[0m in \u001b[0;36m<cell line: 11>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=1'>2</a>\u001b[0m \u001b[39mfrom\u001b[39;00m \u001b[39mpyspark\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39msql\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mtypes\u001b[39;00m \u001b[39mimport\u001b[39;00m StructType, StructField, StringType\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=3'>4</a>\u001b[0m \u001b[39m# schema = StructType(\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=4'>5</a>\u001b[0m \u001b[39m#     [\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=5'>6</a>\u001b[0m \u001b[39m#         StructField('lat', StringType(), True),\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m#         StructField('lon', StringType(), True)\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m \u001b[39m#     ]\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m \u001b[39m# )\u001b[39;00m\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=10'>11</a>\u001b[0m result_df\u001b[39m.\u001b[39;49mwithColumn(\u001b[39m\"\u001b[39;49m\u001b[39mresult\u001b[39;49m\u001b[39m\"\u001b[39;49m, from_json(\u001b[39m\"\u001b[39;49m\u001b[39mresult\u001b[39;49m\u001b[39m\"\u001b[39;49m, schema))\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=11'>12</a>\u001b[0m     \u001b[39m.\u001b[39mselect(col(\u001b[39m'\u001b[39m\u001b[39mresult.*\u001b[39m\u001b[39m'\u001b[39m))\\\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/mouha/Documents/UIDT/Traitement%20De%20Donnees%20De%20Capteurs/Projet_Final_BigData/scripts/get_data.ipynb#X35sZmlsZQ%3D%3D?line=12'>13</a>\u001b[0m     \u001b[39m.\u001b[39mshow()\n",
      "File \u001b[1;32m~\\Spark\\python\\pyspark\\sql\\dataframe.py:4789\u001b[0m, in \u001b[0;36mDataFrame.withColumn\u001b[1;34m(self, colName, col)\u001b[0m\n\u001b[0;32m   4784\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(col, Column):\n\u001b[0;32m   4785\u001b[0m     \u001b[39mraise\u001b[39;00m PySparkTypeError(\n\u001b[0;32m   4786\u001b[0m         error_class\u001b[39m=\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mNOT_COLUMN\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   4787\u001b[0m         message_parameters\u001b[39m=\u001b[39m{\u001b[39m\"\u001b[39m\u001b[39marg_name\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39m\"\u001b[39m\u001b[39mcol\u001b[39m\u001b[39m\"\u001b[39m, \u001b[39m\"\u001b[39m\u001b[39marg_type\u001b[39m\u001b[39m\"\u001b[39m: \u001b[39mtype\u001b[39m(col)\u001b[39m.\u001b[39m\u001b[39m__name__\u001b[39m},\n\u001b[0;32m   4788\u001b[0m     )\n\u001b[1;32m-> 4789\u001b[0m \u001b[39mreturn\u001b[39;00m DataFrame(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_jdf\u001b[39m.\u001b[39;49mwithColumn(colName, col\u001b[39m.\u001b[39;49m_jc), \u001b[39mself\u001b[39m\u001b[39m.\u001b[39msparkSession)\n",
      "File \u001b[1;32m~\\Spark\\python\\lib\\py4j-0.10.9.7-src.zip\\py4j\\java_gateway.py:1322\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1316\u001b[0m command \u001b[39m=\u001b[39m proto\u001b[39m.\u001b[39mCALL_COMMAND_NAME \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcommand_header \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     args_command \u001b[39m+\u001b[39m\\\n\u001b[0;32m   1319\u001b[0m     proto\u001b[39m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1321\u001b[0m answer \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mgateway_client\u001b[39m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1322\u001b[0m return_value \u001b[39m=\u001b[39m get_return_value(\n\u001b[0;32m   1323\u001b[0m     answer, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mgateway_client, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtarget_id, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mname)\n\u001b[0;32m   1325\u001b[0m \u001b[39mfor\u001b[39;00m temp_arg \u001b[39min\u001b[39;00m temp_args:\n\u001b[0;32m   1326\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mhasattr\u001b[39m(temp_arg, \u001b[39m\"\u001b[39m\u001b[39m_detach\u001b[39m\u001b[39m\"\u001b[39m):\n",
      "File \u001b[1;32m~\\Spark\\python\\pyspark\\errors\\exceptions\\captured.py:175\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    171\u001b[0m converted \u001b[39m=\u001b[39m convert_exception(e\u001b[39m.\u001b[39mjava_exception)\n\u001b[0;32m    172\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    173\u001b[0m     \u001b[39m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    174\u001b[0m     \u001b[39m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 175\u001b[0m     \u001b[39mraise\u001b[39;00m converted \u001b[39mfrom\u001b[39;00m \u001b[39mNone\u001b[39m\n\u001b[0;32m    176\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m    177\u001b[0m     \u001b[39mraise\u001b[39;00m\n",
      "\u001b[1;31mAnalysisException\u001b[0m: [DATATYPE_MISMATCH.UNEXPECTED_INPUT_TYPE] Cannot resolve \"from_json(result)\" due to data type mismatch: Parameter 1 requires the \"STRING\" type, however \"result\" has the type \"STRUCT<lat: DOUBLE, lon: DOUBLE, timezone: STRING, timezone_offset: INT, current: STRUCT<dt: BIGINT, sunrise: BIGINT, sunset: BIGINT, temp: DOUBLE, feels_like: DOUBLE, pressure: INT, humidity: INT, dew_point: DOUBLE, uvi: INT, clouds: INT, visibility: INT, wind_speed: DOUBLE, wind_deg: INT, wind_gust: DOUBLE, weather: ARRAY<STRUCT<id: INT, main: STRING, description: STRING, icon: STRING>>>>\".;\n'Project [verb#227, url#228, headers#229, body#230, from_json(StructField(lat,DoubleType,true), StructField(lon,DoubleType,true), StructField(timezone,StringType,true), StructField(timezone_offset,IntegerType,true), StructField(current,StructType(StructField(dt,LongType,true),StructField(sunrise,LongType,true),StructField(sunset,LongType,true),StructField(temp,DoubleType,true),StructField(feels_like,DoubleType,true),StructField(pressure,IntegerType,true),StructField(humidity,IntegerType,true),StructField(dew_point,DoubleType,true),StructField(uvi,IntegerType,true),StructField(clouds,IntegerType,true),StructField(visibility,IntegerType,true),StructField(wind_speed,DoubleType,true),StructField(wind_deg,IntegerType,true),StructField(wind_gust,DoubleType,true),StructField(weather,ArrayType(StructType(StructField(id,IntegerType,true),StructField(main,StringType,true),StructField(description,StringType,true),StructField(icon,StringType,true)),true),true)),true), result#311, Some(Etc/UTC)) AS result#417]\n+- Project [verb#227, url#228, headers#229, body#230, executeRestApi(verb#227, url#228, headers#229, body#230)#310 AS result#311]\n   +- LogicalRDD [verb#227, url#228, headers#229, body#230], false\n"
     ]
    }
   ],
   "source": [
    "import struct\n",
    "from pyspark.sql.functions import from_json, col\n",
    "from pyspark.sql.types import StructType, StructField, StringType\n",
    "\n",
    "# schema = StructType(\n",
    "#     [\n",
    "#         StructField('lat', StringType(), True),\n",
    "#         StructField('lon', StringType(), True)\n",
    "#     ]\n",
    "# )\n",
    "\n",
    "result_df.withColumn(\"result\", struct(\"result\", schema))\\\n",
    "    .select(col('result.*'))\\\n",
    "    .show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
